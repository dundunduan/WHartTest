
"""LangGraphæ™ºèƒ½ç¼–æ’å›¾å®ç° - æ‰€æœ‰Agentéƒ½èƒ½è‡ªä¸»è°ƒç”¨MCPå·¥å…·å’ŒçŸ¥è¯†åº“"""
import json
import logging
from typing import TypedDict, Annotated, List, Literal, Optional
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, AnyMessage
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain.tools import Tool

from .prompts import get_agent_prompt
from .context_compression import CompressionSettings, CompressionResult, ConversationCompressor
from knowledge.models import KnowledgeBase

logger = logging.getLogger(__name__)


class OrchestratorState(TypedDict):
    """ç¼–æ’çŠ¶æ€å®šä¹‰"""
    messages: Annotated[List[AnyMessage], add_messages]
    requirement: str
    project_id: int  # é¡¹ç›®ID,ç”¨äºæ•°æ®éš”ç¦»
    requirement_analysis: dict
    knowledge_docs: list
    testcases: list
    next_agent: str
    instruction: str
    reason: str
    current_step: int
    max_steps: int
    # ä¸Šä¸‹æ–‡å‹ç¼©ç›¸å…³
    context_summary: Optional[str]
    summarized_message_count: int
    context_token_count: int


def create_knowledge_tool(project_id: int, max_retries: int = 3) -> Tool:
    """
    åˆ›å»ºçŸ¥è¯†åº“æœç´¢å·¥å…·ï¼Œè‡ªåŠ¨é‡è¯•æœ€å¤š3æ¬¡
    
    Args:
        project_id: é¡¹ç›®ID
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    Returns:
        LangChain Toolå¯¹è±¡
    """
    def search_knowledge_base(query: str) -> str:
        """
        åœ¨é¡¹ç›®çš„æ‰€æœ‰çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³æ–‡æ¡£ï¼Œè‡ªåŠ¨é‡è¯•æœ€å¤š3æ¬¡
        
        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
        
        Returns:
            æœç´¢ç»“æœæ‘˜è¦ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è¿”å›å¤±è´¥ä¿¡æ¯
        """
        logger.info(f"ğŸ” çŸ¥è¯†åº“å·¥å…·è¢«è°ƒç”¨: query='{query}', max_retries={max_retries}")
        
        for attempt in range(max_retries):
            try:
                # è·å–é¡¹ç›®ä¸‹æ‰€æœ‰æ¿€æ´»çš„çŸ¥è¯†åº“
                project_kbs = KnowledgeBase.objects.filter(
                    project_id=project_id,
                    is_active=True
                )
                
                if not project_kbs.exists():
                    msg = f"é¡¹ç›® {project_id} ä¸‹æ²¡æœ‰å¯ç”¨çš„çŸ¥è¯†åº“"
                    logger.warning(msg)
                    return msg
                
                logger.info(f"  ğŸ“š ç¬¬{attempt+1}æ¬¡å°è¯•: åœ¨ {project_kbs.count()} ä¸ªçŸ¥è¯†åº“ä¸­æœç´¢...")
                
                all_docs = []
                for kb in project_kbs:
                    try:
                        # ä½¿ç”¨VectorStoreManagerè¿›è¡Œæœç´¢
                        from knowledge.services import KnowledgeBaseService
                        kb_service = KnowledgeBaseService(kb)
                        results = kb_service.vector_manager.similarity_search(query, k=3, score_threshold=0.1)
                        if results:
                            all_docs.extend(results)
                            logger.info(f"    â””â”€ {kb.name}: æ‰¾åˆ° {len(results)} ä¸ªæ–‡æ¡£")
                    except Exception as e:
                        logger.error(f"    â””â”€ {kb.name}: æœç´¢å¤±è´¥ {e}")
                
                if all_docs:
                    # æ‰¾åˆ°æ–‡æ¡£ï¼Œè¿”å›æ‘˜è¦
                    docs_summary = "\n\n".join([
                        f"ã€æ–‡æ¡£{i+1}ã€‘æ¥æº: {doc.get('metadata', {}).get('source', 'æœªçŸ¥')}\nå†…å®¹: {doc.get('content', '')[:300]}..."
                        for i, doc in enumerate(all_docs[:5])
                    ])
                    logger.info(f"  âœ… ç¬¬{attempt+1}æ¬¡å°è¯•æˆåŠŸ: æ‰¾åˆ° {len(all_docs)} ä¸ªæ–‡æ¡£")
                    return f"æ‰¾åˆ° {len(all_docs)} ä¸ªç›¸å…³æ–‡æ¡£:\n\n{docs_summary}"
                else:
                    logger.info(f"  âš ï¸ ç¬¬{attempt+1}æ¬¡å°è¯•: æœªæ‰¾åˆ°æ–‡æ¡£")
                    if attempt < max_retries - 1:
                        continue  # ç»§ç»­é‡è¯•
                    else:
                        return f"åœ¨ {project_kbs.count()} ä¸ªçŸ¥è¯†åº“ä¸­æœç´¢äº† {max_retries} æ¬¡ï¼Œæœªæ‰¾åˆ°ä¸'{query}'ç›¸å…³çš„æ–‡æ¡£"
            
            except Exception as e:
                logger.error(f"  âŒ ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥: {e}", exc_info=True)
                if attempt < max_retries - 1:
                    continue
                else:
                    return f"çŸ¥è¯†åº“æœç´¢å¤±è´¥ï¼ˆé‡è¯•{max_retries}æ¬¡ï¼‰: {str(e)}"
        
        return f"çŸ¥è¯†åº“æœç´¢å¤±è´¥: è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°{max_retries}"
    
    return Tool(
        name="search_knowledge_base",
        description=f"åœ¨é¡¹ç›®ID={project_id}çš„æ‰€æœ‰çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³æ–‡æ¡£ã€‚è¾“å…¥æœç´¢æŸ¥è¯¢ï¼Œè¿”å›ç›¸å…³æ–‡æ¡£å†…å®¹ã€‚è‡ªåŠ¨é‡è¯•æœ€å¤š{max_retries}æ¬¡ã€‚é€‚ç”¨äºæŸ¥æ‰¾é¡¹ç›®æ–‡æ¡£ã€éœ€æ±‚ã€è®¾è®¡ç­‰ä¿¡æ¯ã€‚",
        func=search_knowledge_base
    )


class AgentNodes:
    """AgentèŠ‚ç‚¹å®ç° - æ‰€æœ‰Agentéƒ½èƒ½ä½¿ç”¨MCPå·¥å…·å’ŒçŸ¥è¯†åº“"""
    
    def __init__(
        self,
        llm: ChatOpenAI,
        user=None,
        mcp_tools=None,
        project_id=None,
        compression_settings: Optional[CompressionSettings] = None,
        model_name: Optional[str] = None
    ):
        self.llm = llm
        self.user = user
        self.project_id = project_id
        
        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡å‹ç¼©å™¨
        self.context_compressor = ConversationCompressor(
            llm=llm,
            model_name=model_name or getattr(llm, "model_name", "gpt-4o"),
            settings=compression_settings or CompressionSettings(),
        )
        
        # åˆ›å»ºå®Œæ•´çš„å·¥å…·åˆ—è¡¨ï¼šMCPå·¥å…· + çŸ¥è¯†åº“å·¥å…·ï¼ˆç”¨äºå­Agentï¼‰
        self.all_tools = list(mcp_tools or [])
        mcp_tool_count = len(mcp_tools or [])
        
        # ğŸ”§ åˆ›å»ºBrainä¸“ç”¨çš„å·¥å…·åˆ—è¡¨ï¼šåªæœ‰knowledge_baseéªŒè¯å·¥å…·ï¼Œæ²¡æœ‰æ‰§è¡Œç±»å·¥å…·
        self.brain_tools = []
        
        if project_id:
            knowledge_tool = create_knowledge_tool(project_id, max_retries=3)
            self.all_tools.append(knowledge_tool)
            self.brain_tools.append(knowledge_tool)  # Brainåªæœ‰knowledgeå·¥å…·ç”¨äºéªŒè¯
            logger.info(f"âœ… AgentNodesåˆå§‹åŒ–: MCPå·¥å…·={mcp_tool_count}ä¸ª, çŸ¥è¯†åº“å·¥å…·=1ä¸ª, æ€»è®¡={len(self.all_tools)}ä¸ª")
            logger.info(f"   Brainå·¥å…·ï¼šåªæœ‰knowledge_baseï¼ˆç”¨äºéªŒè¯ï¼Œä¸æ˜¯æ‰§è¡Œï¼‰")
            if mcp_tool_count == 0:
                logger.warning(f"âš ï¸ æ³¨æ„ï¼šæ²¡æœ‰MCPå·¥å…·è¢«åŠ è½½ï¼æ‰€æœ‰agentå°†åªèƒ½ä½¿ç”¨çŸ¥è¯†åº“å·¥å…·ã€‚")
            else:
                # åˆ—å‡ºæ‰€æœ‰MCPå·¥å…·åç§°
                mcp_tool_names = [tool.name for tool in mcp_tools] if mcp_tools else []
                logger.info(f"   å¯ç”¨MCPå·¥å…·: {', '.join(mcp_tool_names)}")
        else:
            logger.info(f"âœ… AgentNodesåˆå§‹åŒ–: MCPå·¥å…·={mcp_tool_count}ä¸ª, æ— çŸ¥è¯†åº“å·¥å…·ï¼ˆæœªæŒ‡å®šproject_idï¼‰")
            if mcp_tool_count == 0:
                logger.warning(f"âš ï¸ æ³¨æ„ï¼šæ²¡æœ‰ä»»ä½•å·¥å…·è¢«åŠ è½½ï¼æ‰€æœ‰agentå°†ç›´æ¥ä½¿ç”¨LLMï¼Œæ— å·¥å…·æ”¯æŒã€‚")
    
    def _create_agent_with_tools(self, system_prompt: str) -> create_react_agent:
        """åˆ›å»ºå¸¦å·¥å…·çš„ReAct Agentï¼ˆå·²åºŸå¼ƒï¼Œç›´æ¥åœ¨å„èŠ‚ç‚¹ä¸­åˆ›å»ºAgentï¼‰"""
        logger.warning("âš ï¸ _create_agent_with_toolså·²åºŸå¼ƒï¼Œä¸åº”è¢«è°ƒç”¨")
        if self.all_tools:
            return create_react_agent(self.llm, self.all_tools)
        else:
            # æ²¡æœ‰å·¥å…·æ—¶ï¼Œè¿”å›Noneï¼Œè°ƒç”¨æ–¹ä¼šç›´æ¥ä½¿ç”¨LLM
            return None
    
    async def _prepare_context(self, state: OrchestratorState) -> CompressionResult:
        """å‡†å¤‡ä¸Šä¸‹æ–‡ï¼šæ‰§è¡ŒTokenæ£€æŸ¥å’Œå‹ç¼©"""
        try:
            return await self.context_compressor.prepare(
                messages=state.get("messages", []),
                summary_text=state.get("context_summary"),
                summarized_count=state.get("summarized_message_count", 0),
            )
        except Exception as exc:
            logger.error("ä¸Šä¸‹æ–‡å‹ç¼©å¤±è´¥: %s", exc, exc_info=True)
            return CompressionResult(
                messages=list(state.get("messages", [])),
                summary_message=None,
                state_updates={},
                triggered=False,
                token_count=state.get("context_token_count", 0) or 0,
            )
    
    def _render_history(self, messages: List[AnyMessage]) -> str:
        """æ¸²æŸ“æ¶ˆæ¯å†å²ä¸ºæ–‡æœ¬ï¼ˆç”¨äºBrainçŠ¶æ€åˆ†æï¼‰"""
        history_lines = []
        for msg in messages:
            label = getattr(msg, "additional_kwargs", {}).get("agent", msg.__class__.__name__)
            raw = getattr(msg, "content", "")
            text = raw if isinstance(raw, str) else str(raw)
            if len(text) > 160:
                text = f"{text[:160]}..."
            history_lines.append(f"- [{label}]: {text}")
        return "\n".join(history_lines)
    
    def _get_executed_agents(self, state: OrchestratorState) -> set:
        """è·å–å·²æ‰§è¡Œè¿‡çš„Agenté›†åˆ"""
        executed = set()
        messages = state.get('messages', [])
        for msg in messages:
            if isinstance(msg, AIMessage):
                agent = msg.additional_kwargs.get('agent')
                if agent in ['chat', 'requirement', 'testcase']:
                    executed.add(agent)
        return executed
    
    def _get_last_agent(self, state: OrchestratorState) -> str:
        """è·å–æœ€è¿‘æ‰§è¡Œçš„Agent"""
        messages = state.get('messages', [])
        for msg in reversed(messages):
            if isinstance(msg, AIMessage):
                agent = msg.additional_kwargs.get('agent')
                if agent in ['chat', 'requirement', 'testcase']:
                    return agent
        return None
    
    def _is_chat_intent(self, requirement: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯å¯¹è¯æ„å›¾ï¼ˆè€Œéæµ‹è¯•ä»»åŠ¡ï¼‰"""
        chat_keywords = ['ä½ å¥½', 'æ‚¨å¥½', 'ä»€ä¹ˆ', 'ä»‹ç»', 'åŠŸèƒ½', 'å¸®åŠ©', 'å¦‚ä½•ä½¿ç”¨']
        requirement_lower = requirement.lower()
        return any(keyword in requirement_lower for keyword in chat_keywords)
    
    def _determine_next_agent(self, state: OrchestratorState) -> tuple:
        """ä½¿ç”¨çŠ¶æ€æœºé€»è¾‘ç¡®å®šä¸‹ä¸€ä¸ªAgent
        
        Returns:
            (next_agent, reason): ä¸‹ä¸€ä¸ªagentå’Œé€‰æ‹©åŸå› 
        """
        executed_agents = self._get_executed_agents(state)
        last_agent = self._get_last_agent(state)
        requirement = state.get('requirement', '')
        
        logger.info(f"[çŠ¶æ€æœºå†³ç­–] å·²æ‰§è¡Œ: {executed_agents}, æœ€è¿‘: {last_agent}")
        
        # è§„åˆ™1ï¼šåˆ¤æ–­æ„å›¾ï¼ˆå¯¹è¯ vs æµ‹è¯•ä»»åŠ¡ï¼‰
        is_chat = self._is_chat_intent(requirement)
        
        if is_chat:
            # å¯¹è¯æµç¨‹ï¼šchat â†’ END
            if 'chat' not in executed_agents:
                return 'chat', 'ç”¨æˆ·å’¨è¯¢æˆ–å¯¹è¯ï¼Œè°ƒç”¨chatå›åº”'
            else:
                return 'END', 'å¯¹è¯å·²å®Œæˆ'
        
        # è§„åˆ™2ï¼šæµ‹è¯•ä»»åŠ¡æµç¨‹ï¼šrequirement â†’ testcase â†’ END
        # ä¼˜å…ˆçº§ï¼šrequirement > testcase > END
        
        if 'requirement' not in executed_agents:
            return 'requirement', 'é¦–å…ˆåˆ†ææµ‹è¯•éœ€æ±‚'
        
        if 'testcase' not in executed_agents and state.get('requirement_analysis'):
            return 'testcase', 'éœ€æ±‚åˆ†æå®Œæˆï¼Œç”Ÿæˆæµ‹è¯•ç”¨ä¾‹'
        
        # è§„åˆ™3ï¼šæ‰€æœ‰å¿…è¦æ­¥éª¤å®Œæˆ
        if state.get('requirement_analysis') and state.get('testcases'):
            return 'END', 'æµ‹è¯•ä»»åŠ¡å®Œæˆ'
        
        # è§„åˆ™4ï¼šå¼‚å¸¸æƒ…å†µ - requirementå®Œæˆä½†æ²¡æœ‰åˆ†æç»“æœ
        if 'requirement' in executed_agents and not state.get('requirement_analysis'):
            logger.warning("[çŠ¶æ€æœº] requirementå·²æ‰§è¡Œä½†æ²¡æœ‰åˆ†æç»“æœï¼Œé‡æ–°æ‰§è¡Œ")
            executed_agents.discard('requirement')  # å…è®¸é‡è¯•
            return 'requirement', 'é‡æ–°åˆ†æéœ€æ±‚ï¼ˆä¸Šæ¬¡æ‰§è¡Œæœªäº§ç”Ÿç»“æœï¼‰'
        
        # é»˜è®¤ï¼šç»“æŸ
        return 'END', 'æµç¨‹å®Œæˆ'
    
    async def _generate_instruction_with_llm(self, state: OrchestratorState, next_agent: str) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆç»™å­Agentçš„æŒ‡ä»¤"""
        requirement = state.get('requirement', '')
        
        # ä¸ºä¸åŒAgentç”Ÿæˆä¸åŒçš„æŒ‡ä»¤æç¤º
        if next_agent == 'chat':
            prompt = f"ç”¨æˆ·è¯´ï¼š{requirement}\nè¯·ç”Ÿæˆå‹å¥½çš„å›åº”æŒ‡ä»¤ã€‚"
        elif next_agent == 'requirement':
            prompt = f"æµ‹è¯•éœ€æ±‚ï¼š{requirement}\nè¯·ç”Ÿæˆè¯¦ç»†çš„éœ€æ±‚åˆ†ææŒ‡ä»¤ï¼ŒåŒ…æ‹¬è¦åˆ†æçš„æµ‹è¯•ç‚¹å’Œä¸šåŠ¡è§„åˆ™ã€‚"
        elif next_agent == 'testcase':
            analysis = state.get('requirement_analysis', {})
            prompt = f"éœ€æ±‚åˆ†æç»“æœï¼š{json.dumps(analysis, ensure_ascii=False)}\nè¯·ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ç”ŸæˆæŒ‡ä»¤ï¼Œæ˜ç¡®æµ‹è¯•åœºæ™¯å’Œè¦†ç›–èŒƒå›´ã€‚"
        else:
            return requirement
        
        try:
            response = await self.llm.ainvoke([
                SystemMessage(content="ä½ æ˜¯æµ‹è¯•ç¼–æ’åŠ©æ‰‹ï¼Œè´Ÿè´£ç”Ÿæˆæ¸…æ™°çš„æ‰§è¡ŒæŒ‡ä»¤ã€‚"),
                HumanMessage(content=prompt)
            ])
            instruction = response.content.strip()
            logger.info(f"[LLMæŒ‡ä»¤ç”Ÿæˆ] {next_agent}: {instruction[:100]}...")
            return instruction
        except Exception as e:
            logger.error(f"[LLMæŒ‡ä»¤ç”Ÿæˆå¤±è´¥] {e}, ä½¿ç”¨é»˜è®¤æŒ‡ä»¤")
            return requirement
    
    async def brain_node(self, state: OrchestratorState) -> dict:
        """Brain Agent - ReAct Agentæ¨¡å¼ + çŠ¶æ€æœºçº¦æŸï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
        
        å†³ç­–ç­–ç•¥ï¼š
        1. Brainä½œä¸ºReAct Agentï¼Œèƒ½ä½¿ç”¨å·¥å…·ï¼ˆç‰¹åˆ«æ˜¯search_knowledge_baseï¼‰è¿›è¡ŒéªŒè¯
        2. ä½¿ç”¨çŠ¶æ€æœºé€»è¾‘å¯¹å†³ç­–ç»“æœè¿›è¡Œçº¦æŸå’ŒéªŒè¯
        3. é˜²æ­¢é‡å¤è°ƒåº¦å’Œæ— æ•ˆå¾ªç¯
        """
        logger.info("=== Brain Agentå†³ç­–ï¼ˆReAct + çŠ¶æ€æœºï¼‰===")
        
        # æ‰§è¡Œä¸Šä¸‹æ–‡å‹ç¼©
        compression = await self._prepare_context(state)
        
        current_step = state.get("current_step", 0)
        max_steps = state.get("max_steps", 10)
        
        if current_step >= max_steps:
            logger.warning(f"[Brain] è¾¾åˆ°æœ€å¤§æ­¥éª¤é™åˆ¶ {max_steps}")
            return {"next_agent": "END", "reason": f"è¾¾åˆ°æœ€å¤§æ­¥éª¤{max_steps}", "current_step": current_step}
        
        # è·å–çŠ¶æ€ä¿¡æ¯
        executed_agents = self._get_executed_agents(state)
        last_agent = self._get_last_agent(state)
        requirement = state.get('requirement', '')
        
        # æ„é€ Brainçš„çŠ¶æ€ä¸Šä¸‹æ–‡ï¼ˆå¸®åŠ©LLMç†è§£å½“å‰çŠ¶æ€ï¼‰
        # ğŸ”§ å…³é”®ï¼šåŒ…å«å®Œæ•´çš„å¯¹è¯å†å²ï¼Œè®©Brain LLMè‡ªå·±åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­
        brain_prompt = await get_agent_prompt('brain', self.user)
        
        # æ„å»ºå¯¹è¯å†å²æ‘˜è¦ï¼ˆä½¿ç”¨å‹ç¼©åçš„æ¶ˆæ¯ï¼‰
        messages = state.get('messages', [])
        conversation_history = self._render_history(compression.messages)
        
        state_context = f"""å½“å‰ä»»åŠ¡çŠ¶æ€åˆ†æï¼š

ğŸ“‹ ç”¨æˆ·éœ€æ±‚: {requirement}

ğŸ“Š æ‰§è¡ŒçŠ¶æ€:
- å·²æ‰§è¡Œçš„Agent: {', '.join(executed_agents) if executed_agents else 'æ— '}
- æœ€è¿‘æ‰§è¡Œ: {last_agent or 'æ— '}
- éœ€æ±‚åˆ†æ: {'âœ… å·²å®Œæˆ' if state.get('requirement_analysis') else 'âŒ æœªå®Œæˆ'}
- æµ‹è¯•ç”¨ä¾‹: {'âœ… å·²å®Œæˆ' if state.get('testcases') else 'âŒ æœªå®Œæˆ'}
- å½“å‰æ­¥éª¤: {current_step + 1}/{max_steps}
- ä¸Šä¸‹æ–‡Token: {compression.token_count}/{self.context_compressor.settings.max_context_tokens}

ğŸ“œ å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆå·²è‡ªåŠ¨å‹ç¼©ï¼‰:
{conversation_history}

âš ï¸ é‡è¦è§„åˆ™ï¼ˆçŠ¶æ€æœºçº¦æŸï¼‰:
1. **ç”¨æˆ·åªå’Œä½ å¯¹è¯**ï¼šå­Agentçš„å›å¤æ˜¯ç»™ä½ çœ‹çš„ï¼Œä¸æ˜¯ç»™ç”¨æˆ·çœ‹çš„ã€‚ä½ éœ€è¦åŸºäºå­Agentçš„ç»“æœå‘ç”¨æˆ·å›å¤
2. æµ‹è¯•Agentï¼ˆrequirement/testcaseï¼‰åªèƒ½è°ƒç”¨ä¸€æ¬¡ï¼Œä½†chatå¯ä»¥å¤šæ¬¡è°ƒç”¨ï¼ˆå·²æ‰§è¡Œ: {executed_agents}ï¼‰
3. æ‰§è¡Œé¡ºåº: chat(å¯¹è¯) æˆ– requirement â†’ testcase â†’ END
4. **å…³é”®åˆ¤æ–­**ï¼šæŸ¥çœ‹å¯¹è¯å†å²ï¼Œå¦‚æœchatå·²ç»å›å¤äº†ï¼Œä½ éœ€è¦åŸºäºchatçš„ç»“æœå‘ç”¨æˆ·å›å¤ï¼ˆè¿”å›ENDï¼‰ï¼›å¦‚æœæ˜¯æ–°çš„ç”¨æˆ·é—®é¢˜ï¼Œåˆ™è°ƒç”¨chatè·å–ä¿¡æ¯
5. æµ‹è¯•ä»»åŠ¡ï¼šæŒ‰requirementâ†’testcaseé¡ºåºæ‰§è¡Œï¼Œæ¯ä¸ªåªè°ƒç”¨ä¸€æ¬¡
6. **å·¥å…·ä½¿ç”¨è§„åˆ™**ï¼š
   - âœ… ä½ æœ‰knowledge_baseå·¥å…·ï¼Œå¯ä»¥ç”¨æ¥éªŒè¯ä¿¡æ¯
   - âŒ ä½ æ²¡æœ‰get_project_name_and_idç­‰æ‰§è¡Œç±»å·¥å…·
   - âŒ éœ€è¦æŸ¥è¯¢é¡¹ç›®ã€æ•°æ®ç­‰æ—¶ï¼Œå¿…é¡»æŒ‡æ´¾chat Agentå»åš
   - âœ… ä½ åªè´Ÿè´£éªŒè¯å’Œå†³ç­–ï¼Œä¸è´Ÿè´£æ‰§è¡ŒæŸ¥è¯¢

è¯·åˆ†æå½“å‰çŠ¶æ€å’Œå¯¹è¯å†å²ï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚å¿…é¡»è¿”å›JSONæ ¼å¼:
{{"next_agent": "requirement|testcase|chat|END", "instruction": "ç»™å­Agentçš„å…·ä½“æŒ‡ä»¤", "reason": "é€‰æ‹©ç†ç”±ï¼ˆè¯´æ˜ä½ çš„åˆ¤æ–­è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ˜¯å¦æ£€æŸ¥äº†å¯¹è¯å†å²ï¼‰"}}"""

        try:
            # ğŸ”§ é‡è¦ï¼šBrainåªä½¿ç”¨knowledge_baseéªŒè¯å·¥å…·ï¼Œä¸ä½¿ç”¨æ‰§è¡Œç±»å·¥å…·
            # Brainå¯ä»¥éªŒè¯ä¿¡æ¯ï¼Œä½†ä¸åº”è¯¥è‡ªå·±åŠ¨æ‰‹æŸ¥è¯¢é¡¹ç›®æ•°æ®ç­‰
            if self.brain_tools:
                logger.info(f"Brainä½¿ç”¨ReActæ¨¡å¼ï¼ˆåªæœ‰knowledge_baseéªŒè¯å·¥å…·ï¼‰")
                messages_with_prompt = [SystemMessage(content=brain_prompt), HumanMessage(content=state_context)]
                agent = create_react_agent(self.llm, self.brain_tools)
                result = await agent.ainvoke({"messages": messages_with_prompt})
                
                # æå–æœ€ç»ˆçš„AIå“åº”
                ai_messages = [msg for msg in result['messages'] if isinstance(msg, AIMessage)]
                decision_text = ai_messages[-1].content if ai_messages else ""
            else:
                logger.info("Brainä½¿ç”¨çº¯LLMå†³ç­–ï¼ˆæ²¡æœ‰éªŒè¯å·¥å…·ï¼‰")
                response = await self.llm.ainvoke([
                    SystemMessage(content=brain_prompt),
                    HumanMessage(content=state_context)
                ])
                decision_text = response.content
            
            logger.info(f"[Brain] ReActå“åº”:\n{decision_text}")
            
            # è§£æJSONå†³ç­–
            next_agent = "END"
            instruction = ""
            reason = ""
            
            if "{" in decision_text:
                json_str = decision_text[decision_text.find("{"):decision_text.rfind("}")+1]
                decision = json.loads(json_str)
                next_agent = decision.get("next_agent", "END")
                instruction = decision.get("instruction", "")
                reason = decision.get("reason", "")
            
            # ğŸ”§ çŠ¶æ€æœºéªŒè¯å’Œä¿®æ­£ï¼ˆé˜²æ­¢LLMè¿åè§„åˆ™ï¼‰
            original_next_agent = next_agent
            
            # éªŒè¯1ï¼šé˜²æ­¢é‡å¤è°ƒç”¨ï¼ˆä½†chat Agentä¾‹å¤–ï¼Œå…è®¸å¤šè½®å¯¹è¯ï¼‰
            if next_agent in executed_agents and next_agent not in ['chat', 'END']:
                logger.warning(f"[çŠ¶æ€æœºçº¦æŸ] LLMå°è¯•é‡å¤è°ƒç”¨ {next_agent}ï¼Œè‡ªåŠ¨ä¿®æ­£")
                # æ ¹æ®çŠ¶æ€æœºç¡®å®šæ­£ç¡®çš„next_agent
                corrected_next, corrected_reason = self._determine_next_agent(state)
                next_agent = corrected_next
                reason = f"âš ï¸ çŠ¶æ€æœºä¿®æ­£ï¼š{original_next_agent}å·²æ‰§è¡Œã€‚{corrected_reason}"
            elif next_agent == 'chat' and 'chat' in executed_agents:
                # chatå…è®¸é‡å¤è°ƒç”¨ï¼Œä½†è®°å½•æ—¥å¿—
                logger.info(f"[çŠ¶æ€æœº] å…è®¸chat Agenté‡å¤è°ƒç”¨ï¼ˆå¤šè½®å¯¹è¯ï¼‰")
            
            # éªŒè¯2ï¼šç¡®ä¿æ‰§è¡Œé¡ºåºï¼ˆrequirementå¿…é¡»åœ¨testcaseä¹‹å‰ï¼‰
            if next_agent == 'testcase' and 'requirement' not in executed_agents:
                logger.warning(f"[çŠ¶æ€æœºçº¦æŸ] å¿…é¡»å…ˆæ‰§è¡Œrequirementå†æ‰§è¡Œtestcase")
                next_agent = 'requirement'
                reason = f"âš ï¸ çŠ¶æ€æœºä¿®æ­£ï¼šå¿…é¡»å…ˆåˆ†æéœ€æ±‚æ‰èƒ½ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"
            
            # éªŒè¯3ï¼šä»»åŠ¡å®Œæˆåˆ¤æ–­
            if 'requirement' in executed_agents and 'testcase' in executed_agents:
                if next_agent not in ['END', 'chat']:
                    logger.info(f"[çŠ¶æ€æœºçº¦æŸ] requirementå’Œtestcaseéƒ½å·²å®Œæˆï¼Œè‡ªåŠ¨ç»“æŸ")
                    next_agent = 'END'
                    reason = f"âœ… æµ‹è¯•ä»»åŠ¡å·²å®Œæˆï¼ˆéœ€æ±‚åˆ†æå’Œæµ‹è¯•ç”¨ä¾‹éƒ½å·²ç”Ÿæˆï¼‰"
            
            # æ„é€ å†³ç­–æ¶ˆæ¯
            mode_label = "æ™ºèƒ½åè°ƒå†³ç­–"
            if original_next_agent != next_agent:
                mode_label += f" [å·²ä¿®æ­£: {original_next_agent}â†’{next_agent}]"
            
            formatted_decision = f"""ğŸ§  Brainå†³ç­–ï¼ˆ{mode_label}ï¼‰: {next_agent}

ğŸ’¡ æŒ‡ä»¤: {instruction if instruction else 'æ— '}

ğŸ“ æ¨ç†: {reason}

ğŸ“Š å½“å‰çŠ¶æ€:
- å·²æ‰§è¡Œ: {', '.join(executed_agents) if executed_agents else 'æ— '}
- éœ€æ±‚åˆ†æ: {'âœ…' if state.get('requirement_analysis') else 'â³'}
- æµ‹è¯•ç”¨ä¾‹: {'âœ…' if state.get('testcases') else 'â³'}
- æ‰§è¡Œæ­¥éª¤: {current_step + 1}/{max_steps}"""
            
            brain_decision = AIMessage(
                content=formatted_decision,
                additional_kwargs={
                    "agent": "brain",
                    "agent_type": "orchestrator_brain_decision",
                    "next_agent": next_agent,
                    "instruction": instruction,
                    "reason": reason,
                    "decision_mode": "react_with_state_machine",
                    "was_corrected": original_next_agent != next_agent,
                    "original_decision": original_next_agent,
                    "is_thinking_process": True  # ğŸ¨ æ ‡è®°ä¸ºæ€è€ƒè¿‡ç¨‹ï¼Œå‰ç«¯é»˜è®¤æŠ˜å 
                }
            )
            
            messages_to_add = [brain_decision]
            
            # ğŸ”§ å…³é”®ï¼šå½“next_agentä¸ºENDæ—¶ï¼Œç”Ÿæˆæœ€ç»ˆç”¨æˆ·å›å¤
            if next_agent == "END":
                logger.info("[Brain] ç”Ÿæˆæœ€ç»ˆç”¨æˆ·å›å¤")
                
                # æ”¶é›†å­Agentçš„æŠ¥å‘Š
                agent_reports = []
                for msg in messages:
                    if isinstance(msg, AIMessage) and msg.additional_kwargs.get('agent') in ['chat', 'requirement', 'testcase']:
                        agent_name = msg.additional_kwargs.get('agent')
                        agent_reports.append(f"[{agent_name}]:\n{msg.content}\n")
                
                reports_summary = "\n".join(agent_reports) if agent_reports else "æ— å­AgentæŠ¥å‘Š"
                
                # è®©BrainåŸºäºå­AgentæŠ¥å‘Šç”Ÿæˆæœ€ç»ˆç”¨æˆ·å›å¤
                final_response_prompt = f"""ä½ ç°åœ¨éœ€è¦ç”Ÿæˆæœ€ç»ˆçš„ç”¨æˆ·å›å¤ã€‚

å­Agentçš„å†…éƒ¨æŠ¥å‘Šï¼š
{reports_summary}

ç”¨æˆ·éœ€æ±‚ï¼š{requirement}

è¯·åŸºäºå­Agentçš„æŠ¥å‘Šï¼Œç”Ÿæˆä¸€ä¸ªæ¸…æ™°ã€å‹å¥½ã€ä¸“ä¸šçš„å›å¤ç»™ç”¨æˆ·ã€‚
- å¦‚æœæ˜¯å¯¹è¯ä»»åŠ¡ï¼Œä½¿ç”¨chat Agentå»ºè®®çš„å›å¤å†…å®¹
- å¦‚æœæ˜¯æµ‹è¯•ä»»åŠ¡ï¼Œæ€»ç»“éœ€æ±‚åˆ†æå’Œæµ‹è¯•ç”¨ä¾‹ç»“æœ
- å›å¤è¦ç›´æ¥é¢å‘ç”¨æˆ·ï¼Œä¸è¦æåŠ"Brain"ã€"å­Agent"ç­‰å†…éƒ¨æ¦‚å¿µ
- ä¿æŒå‹å¥½ã€ç®€æ´ã€ä¸“ä¸šçš„è¯­æ°”"""

                final_response = await self.llm.ainvoke([
                    SystemMessage(content="ä½ æ˜¯æµ‹è¯•åŠ©æ‰‹ï¼Œè´Ÿè´£å‘ç”¨æˆ·æä¾›å‹å¥½ã€ä¸“ä¸šçš„å›å¤"),
                    HumanMessage(content=final_response_prompt)
                ])
                
                # åˆ›å»ºæœ€ç»ˆç”¨æˆ·å›å¤æ¶ˆæ¯
                user_response_message = AIMessage(
                    content=final_response.content,
                    additional_kwargs={
                        "agent": "brain",
                        "agent_type": "orchestrator_final_response",
                        "is_final_response": True
                    }
                )
                
                messages_to_add.append(user_response_message)
                logger.info(f"[Brain] æœ€ç»ˆç”¨æˆ·å›å¤:\n{final_response.content}")
            
            result_payload = {
                "next_agent": next_agent,
                "instruction": instruction,
                "reason": reason,
                "messages": messages_to_add,
                "current_step": current_step + 1,
                "executed_agents": executed_agents,
                "requirement_analysis": state.get('requirement_analysis'),
                "testcases": state.get('testcases'),
                "max_steps": max_steps
            }
            # åˆå¹¶å‹ç¼©çŠ¶æ€æ›´æ–°
            result_payload.update(compression.state_updates)
            return result_payload
            
        except Exception as e:
            logger.error(f"[Brain] å†³ç­–å¤±è´¥: {e}", exc_info=True)
            # fallbackåˆ°çŠ¶æ€æœºå†³ç­–
            next_agent, reason = self._determine_next_agent(state)
            instruction = state.get('requirement', '')
            
            brain_decision = AIMessage(
                content=f"ğŸ§  Brainå†³ç­–ï¼ˆfallbackçŠ¶æ€æœºï¼‰: {next_agent}\n\nğŸ“ æ¨ç†: {reason}\n\nâš ï¸ æ³¨æ„ï¼šLLMå†³ç­–å¤±è´¥ï¼Œä½¿ç”¨çŠ¶æ€æœºfallback",
                additional_kwargs={
                    "agent": "brain",
                    "agent_type": "orchestrator_brain_decision",
                    "next_agent": next_agent,
                    "instruction": instruction,
                    "reason": reason,
                    "decision_mode": "fallback_state_machine",
                    "is_thinking_process": True  # ğŸ¨ æ ‡è®°ä¸ºæ€è€ƒè¿‡ç¨‹ï¼Œå‰ç«¯é»˜è®¤æŠ˜å 
                }
            )
            
            fallback_result = {
                "next_agent": next_agent,
                "instruction": instruction,
                "reason": reason,
                "messages": [brain_decision],
                "current_step": current_step + 1,
                "executed_agents": state.get('executed_agents', []),
                "requirement_analysis": state.get('requirement_analysis'),
                "testcases": state.get('testcases'),
                "max_steps": state.get('max_steps', 10)
            }
            fallback_result.update(compression.state_updates)
            return fallback_result
    
    async def chat_node(self, state: OrchestratorState) -> dict:
        """Chat Agent - æ”¯æŒMCPå·¥å…·å’ŒçŸ¥è¯†åº“ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        logger.info("=== Chat Agentå¤„ç†å¯¹è¯ ===")
        
        compression = await self._prepare_context(state)
        prompt = await get_agent_prompt('chat')
        instruction = state.get('instruction') or state.get('requirement')
        
        try:
            if self.all_tools:
                logger.info(f"Chat Agentä½¿ç”¨ {len(self.all_tools)} ä¸ªå·¥å…·")
                messages_with_prompt = [SystemMessage(content=prompt)] + compression.messages
                agent = create_react_agent(self.llm, self.all_tools)
                result = await agent.ainvoke({"messages": messages_with_prompt})
                
                ai_messages = [msg for msg in result['messages'] if isinstance(msg, AIMessage)]
                response_content = ai_messages[-1].content if ai_messages else "Chat Agentæœªè¿”å›æœ‰æ•ˆå“åº”"
            else:
                logger.info("Chat Agentç›´æ¥ä½¿ç”¨LLM")
                response = await self.llm.ainvoke([SystemMessage(content=prompt), HumanMessage(content=instruction)])
                response_content = response.content
            
            chat_message = AIMessage(
                content=response_content,
                additional_kwargs={
                    "agent": "chat",
                    "agent_type": "orchestrator_agent",
                    "is_thinking_process": True
                }
            )
            
            result = dict(compression.state_updates)
            result["messages"] = [chat_message]
            return result
        except Exception as e:
            logger.error(f"å¯¹è¯å¤„ç†å¤±è´¥: {e}", exc_info=True)
            result = dict(compression.state_updates)
            result["messages"] = [AIMessage(content=f"å¯¹è¯å¤±è´¥: {e}")]
            return result
    
    async def requirement_node(self, state: OrchestratorState) -> dict:
        """Requirement Agent - æ”¯æŒMCPå·¥å…·å’ŒçŸ¥è¯†åº“ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        logger.info("=== Requirement Agentåˆ†æéœ€æ±‚ ===")
        
        compression = await self._prepare_context(state)
        prompt = await get_agent_prompt('requirement')
        instruction = state.get('instruction') or state.get('requirement')
        
        try:
            if self.all_tools:
                logger.info(f"Requirement Agentä½¿ç”¨ {len(self.all_tools)} ä¸ªå·¥å…·")
                messages_with_prompt = [SystemMessage(content=prompt)] + compression.messages
                agent = create_react_agent(self.llm, self.all_tools)
                result = await agent.ainvoke({"messages": messages_with_prompt})
                
                ai_messages = [msg for msg in result['messages'] if isinstance(msg, AIMessage)]
                analysis_text = ai_messages[-1].content if ai_messages else "Requirement Agentæœªè¿”å›æœ‰æ•ˆå“åº”"
            else:
                logger.info("Requirement Agentç›´æ¥ä½¿ç”¨LLM")
                response = await self.llm.ainvoke([SystemMessage(content=prompt), HumanMessage(content=f"åˆ†æéœ€æ±‚:\n{instruction}")])
                analysis_text = response.content
            
            # è§£æåˆ†æç»“æœ
            if "{" in analysis_text:
                json_str = analysis_text[analysis_text.find("{"):analysis_text.rfind("}")+1]
                analysis = json.loads(json_str)
            else:
                analysis = {"åŠŸèƒ½æè¿°": instruction, "åˆ†æç»“æœ": analysis_text}
            
            requirement_message = AIMessage(
                content=analysis_text,
                additional_kwargs={
                    "agent": "requirement",
                    "agent_type": "orchestrator_agent",
                    "is_thinking_process": True
                }
            )
            
            result = dict(compression.state_updates)
            result.update({"requirement_analysis": analysis, "messages": [requirement_message]})
            return result
        except Exception as e:
            logger.error(f"éœ€æ±‚åˆ†æå¤±è´¥: {e}", exc_info=True)
            result = dict(compression.state_updates)
            result.update({"requirement_analysis": {"error": str(e)}, "messages": [AIMessage(content=f"åˆ†æå¤±è´¥: {e}")]})
            return result
    

    
    async def testcase_node(self, state: OrchestratorState) -> dict:
        """TestCase Agent - æ”¯æŒMCPå·¥å…·å’ŒçŸ¥è¯†åº“ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        logger.info("=== TestCase Agentç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ ===")
        
        compression = await self._prepare_context(state)
        prompt = await get_agent_prompt('testcase')
        requirement_analysis = state.get('requirement_analysis', {})
        knowledge_docs = state.get('knowledge_docs', [])
        
        context = f"""éœ€æ±‚åˆ†æ:
{json.dumps(requirement_analysis, ensure_ascii=False, indent=2)}

çŸ¥è¯†æ–‡æ¡£: {len(knowledge_docs)}ä¸ª

è¯·ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹(JSONæ ¼å¼)ã€‚
"""
        
        try:
            if self.all_tools:
                logger.info(f"TestCase Agentä½¿ç”¨ {len(self.all_tools)} ä¸ªå·¥å…·")
                messages_with_prompt = [SystemMessage(content=prompt)] + compression.messages
                agent = create_react_agent(self.llm, self.all_tools)
                result = await agent.ainvoke({"messages": messages_with_prompt})
                
                ai_messages = [msg for msg in result['messages'] if isinstance(msg, AIMessage)]
                testcase_text = ai_messages[-1].content if ai_messages else "TestCase Agentæœªè¿”å›æœ‰æ•ˆå“åº”"
            else:
                logger.info("TestCase Agentç›´æ¥ä½¿ç”¨LLM")
                response = await self.llm.ainvoke([SystemMessage(content=prompt), HumanMessage(content=context)])
                testcase_text = response.content
            
            # è§£ææµ‹è¯•ç”¨ä¾‹
            if "{" in testcase_text:
                json_str = testcase_text[testcase_text.find("{"):testcase_text.rfind("}")+1]
                testcases_data = json.loads(json_str)
                testcases = testcases_data.get("æµ‹è¯•ç”¨ä¾‹", [])
            else:
                testcases = [{"å†…å®¹": testcase_text}]
            
            testcase_message = AIMessage(
                content=testcase_text,
                additional_kwargs={
                    "agent": "testcase",
                    "agent_type": "orchestrator_agent",
                    "is_thinking_process": True
                }
            )
            
            result = dict(compression.state_updates)
            result.update({"testcases": testcases, "messages": [testcase_message]})
            return result
        except Exception as e:
            logger.error(f"æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            result = dict(compression.state_updates)
            result.update({"testcases": [], "messages": [AIMessage(content=f"ç”Ÿæˆå¤±è´¥: {e}")]})
            return result


def create_orchestrator_graph(
    llm: ChatOpenAI,
    checkpointer=None,
    user=None,
    mcp_tools=None,
    project_id=None,
    compression_settings: Optional[CompressionSettings] = None,
    model_name: Optional[str] = None
) -> StateGraph:
    """åˆ›å»ºæ™ºèƒ½ç¼–æ’å›¾
    
    Args:
        llm: LLMå®ä¾‹
        checkpointer: å¯é€‰çš„checkpointer,ç”¨äºä¿å­˜å¯¹è¯å†å²
        user: å¯é€‰çš„ç”¨æˆ·å¯¹è±¡,ç”¨äºè·å–ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯
        mcp_tools: å¯é€‰çš„MCPå·¥å…·åˆ—è¡¨
        project_id: é¡¹ç›®IDï¼Œç”¨äºåˆ›å»ºçŸ¥è¯†åº“å·¥å…·
        compression_settings: ä¸Šä¸‹æ–‡å‹ç¼©é…ç½®
        model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºTokenè®¡æ•°ï¼‰
    """
    
    nodes = AgentNodes(
        llm,
        user,
        mcp_tools,
        project_id,
        compression_settings=compression_settings,
        model_name=model_name
    )
    workflow = StateGraph(OrchestratorState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("brain_agent", nodes.brain_node)
    workflow.add_node("chat_agent", nodes.chat_node)
    workflow.add_node("requirement_agent", nodes.requirement_node)
    workflow.add_node("testcase_agent", nodes.testcase_node)
    
    # è·¯ç”±å‡½æ•°
    def router(state: OrchestratorState) -> Literal["chat_agent", "requirement_agent", "testcase_agent", "__end__"]:
        next_agent = state.get("next_agent", "END")
        if next_agent == "chat":
            return "chat_agent"
        elif next_agent == "requirement":
            return "requirement_agent"
        elif next_agent == "testcase":
            return "testcase_agent"
        else:
            return "__end__"
    
    # æ¡ä»¶è¾¹
    workflow.add_conditional_edges(
        "brain_agent",
        router,
        {
            "chat_agent": "chat_agent",
            "requirement_agent": "requirement_agent",
            "testcase_agent": "testcase_agent",
            "__end__": END
        }
    )
    
    # æ‰€æœ‰å­Agentè¿”å›Brainç»§ç»­å†³ç­–ï¼ˆè®©Brainåè°ƒæ•´ä¸ªæµç¨‹ï¼‰
    workflow.add_edge("chat_agent", "brain_agent")
    workflow.add_edge("requirement_agent", "brain_agent")
    workflow.add_edge("testcase_agent", "brain_agent")
    
    # å…¥å£ç‚¹
    workflow.set_entry_point("brain_agent")
    
    # ç¼–è¯‘æ—¶ä¼ å…¥checkpointer
    return workflow.compile(checkpointer=checkpointer)
